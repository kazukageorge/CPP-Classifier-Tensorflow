%{
% added codes to tell the machine to pick which dataset index are used for
% training, veri and testing
% added random variables to the verification
% THIS CODE HAS THE DISTANCE PARAMETER, TOTALDISP AND MAX_MSD IN,
% BECAUSE WE ARE CHECKING IF HAVING RANDOM VERIFI CHANGES. 
% IF ONLY NON-DIST PARA WANTED, UNCOMMENT LINE 39, COMMENT LINE 36

% if the you get error:

Error using nnetParamInfo (line 28)
SDPVAR/LOG called with CHAR argument?

type in 
restoredefaultpath
rehash toolboxcache

% --- --- --- --- -- --- -- --- -- -- --- --- -
% Load the organized data structure 
% KGO 4/4/19, Matlab ver. 2017b
%}
%%
clear; clc; format shortG;
load(fullfile('AuxClassificationInput')); %df = datafile
continuousLabels = {'TrackNum', 'aux', 'lifetime', 'max_intensity', 'background', 'totaldisp', 'max_msd', 'avg_rise', 'avg_dec', 'risevsdec', 'avg_mom_rise', 'avg_mom_dec', 'risevsdec_mom'};
% ---  input data from 1 to 8
% ---  random_x = x(randperm(size(x, 1)), :
% inputData = [df(1).aux',df(1).lifetime',df(1).intensity_max',
% df(1).bkground_scal',df(1).tdisp_scal',df(1).msd_max',df(1).avgRise',df(1).avgDecay',df(1).riseVsDecay',df(1).avgRiseMom',df(1).avgDecayMom',df(1).riseVsDecayMom']
%     inputData = [ inputData   ; [string(df(ii).trackNum'), string(df(ii).aux'),string(df(ii).lifetime'),string(df(ii).intensity_max'), string(df(ii).bkground_scal'),string(df(ii).tdisp_scal'),string(df(ii).msd_max'),string(df(ii).avgRise'),string(df(ii).avgDecay'),string(df(ii).riseVsDecay'),string(df(ii).avgRiseMom'),string(df(ii).avgDecayMom'),string(df(ii).riseVsDecayMom')] ];

inputData = [];
for ii = 1:8 
    % append the data by using inputData
    % With distance parameters
%             inputData = [ inputData   ; [string(df(ii).trackNum'), df(ii).aux',df(ii).lifetime',df(ii).intensity_max', df(ii).bkground_scal',df(ii).tdisp_scal',df(ii).msd_max',df(ii).avgRise',df(ii).avgDecay',df(ii).riseVsDecay',df(ii).avgRiseMom',df(ii).avgDecayMom',df(ii).riseVsDecayMom'] ];
               inputData = [ inputData   ; [(df(ii).trackNum'), df(ii).aux',df(ii).lifetime',df(ii).intensity_max', df(ii).bkground_scal',df(ii).tdisp_scal',df(ii).msd_max',df(ii).avgRise',df(ii).avgDecay',df(ii).riseVsDecay',df(ii).avgRiseMom',df(ii).avgDecayMom',df(ii).riseVsDecayMom'] ];

    % wout distnace para
%     inputData = [ inputData   ; [string(df(ii).trackNum'), df(ii).aux',df(ii).lifetime',df(ii).intensity_max', df(ii).bkground_scal',df(ii).avgRise',df(ii).avgDecay',df(ii).riseVsDecay',df(ii).avgRiseMom',df(ii).avgDecayMom',df(ii).riseVsDecayMom'] ];
end
disp('First 10 samples (in String) before randomizing them');
disp(continuousLabels);
disp(inputData(1:10,:))

rinputData = inputData(randperm(size(inputData, 1)),:); % randomize the dataset (only rows)
trainingIndex = 1:ceil(length(rinputData)*0.64); %
validationIndex = (length(trainingIndex) + 1):( ceil(length(rinputData)*0.16) + length(trainingIndex));
testingIndex = (length(validationIndex) + length(trainingIndex) + 1) : length(rinputData);


% randomize the inputs 
% trainingIndex = 1:ceil(length(rinputData)*0.64);
% validationIndex = (length(trainingIndex) + 1):( ceil(length(rinputData)*0.16) + length(trainingIndex));
% testingIndex = (length(validationIndex) + length(trainingIndex) + 1) : length(rinputData);
% 
% trainingDataString = rinputData(trainingIndex,:);
% validataionDataString = rinputData(validationIndex,:);
% testingDataString = rinputData(testingIndex,:);
% 
% trainingData = double(trainingDataString(:,2:end)); % omit the trackNum 
% validataionData = double(validataionDataString(:,2:end)); % omit the trackNum 
% testingData = double(testingDataString(:,2:end)); % omit the trackNum

allData = double(rinputData(:,2:end));

% aux = allData(:,1);
% lifetime = allData(:,2);
% max_intensity = allData(:,3);
% background = allData(:,4);
% totaldisp = allData(:,5);
% % max_msd = allData(:,6);
% % avg_rise = allData(:,7);
% avg_dec = allData(:,8);
% risevsdec = allData(:,9);
% avg_mom_rise = allData(:,10);
% avg_mom_dec = allData(:,11);
% risevsdec_mom = allData(:,12);
% % 
% trainingDataTable = table(aux, lifetime, max_intensity, background, totaldisp, max_msd, avg_rise, avg_dec, risevsdec, avg_mom_rise, avg_mom_dec, risevsdec_mom);

% features = [lifetime max_intensity background totaldisp max_msd avg_rise avg_dec risevsdec avg_mom_rise avg_mom_dec risevsdec_mom];
features = allData(:,2:end);

predictors = allData(:,1);
predictors = zeros(size(predictors));
%%

% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 28-Mar-2019 11:43:16
%
% This script assumes these variables are defined:
%
%   features - input data.
%   t - target data.

x = features';
t = zeros(2,numel(predictors));
for i = 1:numel(predictors)
    if (predictors(i))
        t(1,i) = 1; % Auz+, observed
    else
        t(2,i) = 1; % Aux-, observed
    end
end

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
% trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

trainFcn =  'trainscg';
% net.divideFcn = 'divideind';
% Create a Pattern Recognition Network
hiddenLayerSize = 10; % default
net = patternnet(hiddenLayerSize, trainFcn);

 net.divideFcn = 'divideind'; % divide the features matrix into train, ver, test
 net.divideParam.trainInd = trainingIndex;
 net.divideParam.valInd   = validationIndex;
 net.divideParam.testInd  = testingIndex;
 
 
 % --- --- --- ---- --- --- --- --- --- - 
 % to use make the validation index to a random number run the code below
 
% x(1:size(x,1), validationIndex) = rand(size(x,1),length(validationIndex));

 % --- --- --- ---- --- --- --- --- --- - 

% Setup Division of Data for Training, Validation, Testing
% net.divideParam.trainRatio = 64/100; % same as jeremy's
% net.divideParam.valRatio = 16/100;
% net.divideParam.testRatio = 20/100;

% net.trainParam.lr = .0001;
% net.trainParam.epochs = 100000;

% Train the Network
tic
a = [];
for ii = 1:10
[net,tr] = train(net,x,t);

% Test the Network
y = net(x); % This gives you the probability of getting Auz+ or -
e = gsubtract(t,y);
performance = perform(net,t,y);
tind = vec2ind(t); % 1 = Aux+, 2 = Aux- from the Obs data 

% Find the classification by selecting the higher probability of selecting
% Aux+/-
[~, yind] = max(y);
predicted = yind-1; %convert indicies to Aux1 or Aux0
percentErrors = sum(tind(testingIndex) ~= yind(testingIndex))/numel(tind(testingIndex)); % only check the accuracy for the testing index. 
trainTargets = t .* tr.trainMask{1};
valTargets   = t .* tr.valMask{1};
testTargets  = t .* tr.testMask{1};
trainPerformance = perform(net,trainTargets,y); % PERFORMANCE TELLS THE CROSS-ENTROY (BECAUSE THIS IS THE MODEL WE ARE USING) LOSS, THIS DEPENDS ON THE LOSS FUCNTION <https://en.wikipedia.org/wiki/Loss_function>, CROSS ENTROPY LOSS: <https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html>
valPerformance   = perform(net,valTargets,y);
testPerformance  = perform(net,testTargets,y);
 
fprintf('training performance is: %g\n', trainPerformance);
fprintf('validation performance is: %g\n', valPerformance);
fprintf('testing performance is: %g\n', testPerformance);

fprintf('Accuracy is %g percent\n',100*(1-percentErrors));
a(ii) = 100*(1-percentErrors);
end
mean(a)
toc



% View the Network
% view(net)
% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotconfusion(t,y)
%figure, plotroc(t,y)


